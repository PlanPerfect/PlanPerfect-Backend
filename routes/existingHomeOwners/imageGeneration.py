"""
Image Generation API

This file handles image generation using img2img Stable Diffusion
It takes a input image and selected styles by the user, 
generates a text prompt using a LLM and then runs Stable Diffusion to generate
a new image

Workflow:
1. Receives input image and style preferences from frontend
2. Uses LLM to generate a prompt for Stable Diffusion based on the selected style(s)
3. Runs Stable Diffusion img2img to generate a new image
4. Returns the generated image file in the response

This API is used by the image generation feature in the frontend

Workflow Design Notes:
- Stable Diffusion parameters are fixed to balance image quality, realism and runtime
- img2img is used to preserve room layout and structure, instead of text-to-image
- Additional checks are added to ensure that LLM output is valid
- Original aspect ratio is preserved throughout the generation process
"""

from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from fastapi.responses import FileResponse
from fastapi.concurrency import run_in_threadpool
from PIL import Image
import uuid
import torch
from Services.StableDiffusion import pipe
from Services.LLMGemini import generate_sd_prompt

def resize_to_target_with_aspect_ratio(image: Image.Image, target_size: int = 512):
    """
    Resize image to target size while maintaining aspect ratio.
    The longest side will be resized to target_size, and the shorter side
    will be scaled proportionally. Both dimensions will be multiples of 8
    (required by Stable Diffusion).
    
    Args:
        image: PIL Image to resize
        target_size: target size for the longest dimension
        
    Returns:
        Resized PIL Image with preserved aspect ratio
    """
    w, h = image.size
    aspect_ratio = w / h
    
    if w >= h:
        # Landscape or square
        new_w = target_size
        new_h = int(target_size / aspect_ratio)
    else:
        # Portrait
        new_h = target_size
        new_w = int(target_size * aspect_ratio)
    
    # Round to nearest multiple of 8 (SD requirement)
    new_w = max(8, (new_w // 8) * 8)
    new_h = max(8, (new_h // 8) * 8)
    
    # Ensure both dimensions are at least 384 but maintain aspect ratio
    if new_w < 384 or new_h < 384:
        scale = max(384 / new_w, 384 / new_h)
        new_w = int(new_w * scale)
        new_h = int(new_h * scale)
        # Round again to multiple of 8
        new_w = (new_w // 8) * 8
        new_h = (new_h // 8) * 8
    
    print(f"Resizing from {w}x{h} to {new_w}x{new_h} (aspect ratio: {new_w/new_h:.2f})")
    
    return image.resize((new_w, new_h), Image.LANCZOS)


def choose_base_resolution(image: Image.Image):
    """
    Decide a safe base resolution for SD img2img
    based on input image size.
    """
    w, h = image.size
    min_side = min(w, h)

    if min_side >= 768:
        base = 512
    elif min_side >= 512:
        base = 448
    else:
        base = 384

    return base


def run_stable_diffusion(init_image: Image.Image, prompt: str):
    """
    Run img2img Stable Diffusion with predefined generation parameters

    Args:
        init_image: room image uploaded by user (with preserved aspect ratio)
        prompt: text promp generated by LLM based on user selected style(s)

    Returns:
        Generated image based on the input image and prompt

    Generation Parameters (strength, guidance_scale) Explanation:
    These parameter values were chosen through experimentation to balance 
    generation time, image quality, realism for interior design and level of style influence.
    
    strength is set to 0.45 to allow significant style changes while preserving room layout.
    guidance_scale of 7.5 increases adherence to the prompt.
    Negative prompt is fixed, as LLM's negative prompts are often low quality, describing 
    generic bad image features. Hence, using a higher quality fixed negative prompt 
    would yield higher image quality.
    """
    return pipe(
        prompt=prompt + ", real interior photograph, natural proportions",
        negative_prompt=(
            "cartoon, anime, illustration, painting, sketch, lowres, blurry, "
            "distorted geometry, warped walls, bent lines, duplicated furniture, "
            "bad perspective, fisheye distortion, oversharpened, noise, jpeg artifacts"
        ),
        image=init_image,
        strength=0.45,
        guidance_scale=7.5,
        num_inference_steps=25
    ).images[0]

router = APIRouter(prefix="/image", tags=["Image Generation"])

@router.post("/generate")
async def generate_image(
    file: UploadFile = File(...),
    styles: str = Form(...),
):
    try:
        # Save input image
        input_path = f"static/input_{uuid.uuid4()}.png"
        with open(input_path, "wb") as f:
            f.write(await file.read())

        init_image = Image.open(input_path).convert("RGB")
        original_size = init_image.size  # Store original dimensions

        # Generate prompt using LLM with retry logic
        max_retries = 3
        parsed = None

        for attempt in range(max_retries):
            try:
                llm_response = generate_sd_prompt(styles)
                parsed = llm_response
                if "prompt" not in parsed:
                    raise ValueError("Missing required fields in JSON")
                break
            except ValueError as e:
                print(f"Attempt {attempt + 1} failed: {str(e)}")
                if attempt == max_retries - 1:
                    # Use fallback prompts
                    print("All retries failed, using fallback prompts")
                    parsed = {
                        "prompt": f"{styles}, detailed, high quality, professional",
                        "negative": "cartoon, anime, illustration, painting, sketch, lowres, blurry, "
                                    "distorted geometry, warped walls, bent lines, duplicated furniture, "
                                    "bad perspective, fisheye distortion, oversharpened, noise, jpeg artifacts"
                    }
                    break

        prompt = parsed["prompt"]

        # Ensure prompts aren't empty
        if not prompt or prompt.strip() == "":
            prompt = "detailed, high quality, professional photograph"
        
        # Clear GPU cache before generation, helps with faster genration of images
        torch.cuda.empty_cache()

        base_size = choose_base_resolution(init_image)

        # Resize while maintaining aspect ratio
        init_image = resize_to_target_with_aspect_ratio(init_image, base_size)

        # Run Stable Diffusion img2img
        result = await run_in_threadpool(run_stable_diffusion, init_image, prompt)

        # Upscale 2x while maintaining aspect ratio
        hires_image = result.resize(
            (result.width * 2, result.height * 2),
            Image.LANCZOS
        )

        hires_result = await run_in_threadpool(
            lambda: pipe(
                prompt=prompt + ", ultra detailed interior photograph, sharp focus, architectural photography",
                negative_prompt=(
                    "cartoon, anime, illustration, distorted geometry, warped walls, "
                    "duplicated furniture, bad perspective"
                ),
                image=hires_image,
                strength=0.20,         
                guidance_scale=7.0,
                num_inference_steps=30
            ).images[0]
        )
        
        output_path = f"static/output_{uuid.uuid4()}.png"
        hires_result.save(output_path, quality=95)  # Higher quality PNG

        return FileResponse(output_path)

    except Exception as e:
        print(f"Error in generate_image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))