"""
Image Generation API

This file handles image generation using img2img Stable Diffusion
It takes a input image and selected styles by the user, 
generates a text prompt using a LLM and then runs Stable Diffusion to generate
a new image

Workflow:
1. Receives input image and style preferences from frontend
2. Uses LLM to generate a prompt for Stable Diffusion based on the selected style(s)
3. Runs Stable Diffusion img2img to generate a new image
4. Returns the generated image file in the response

This API is used by the image generation feature in the frontend

Workflow Design Notes:
- Stable Diffusion parameters are fixed to balance image quality, realism and runtime
- img2img is used to preserve room layout and structure, instead of text-to-image
- Additional checks are added to ensure that LLM output is valid
"""

from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from fastapi.responses import FileResponse
from fastapi.concurrency import run_in_threadpool
from PIL import Image
import uuid
import torch
from Services.StableDiffusion import pipe
from Services.LLMGemini import generate_sd_prompt

def choose_base_resolution(image: Image.Image):
    """
    Decide a safe base resolution for SD img2img
    based on input image size and aspect ratio.
    """
    w, h = image.size
    min_side = min(w, h)

    if min_side >= 768:
        base = 512
    elif min_side >= 512:
        base = 448
    else:
        base = 384

    return base


def run_stable_diffusion(init_image: Image.Image, prompt: str):
    """
    Run img2img Stable Diffusion with predefined generation parameters

    Args:
        init_image: room image uploaded by user
        prompt: text promp generated by LLM based on user selected style(s)

    Returns:
        Generated image based on the input image and prompt

    Generation Parameters (strength, guidance_scale) Explanation:
    These parameter values were chosen through experimentation to balance 
    generation time, image quality, realism for interior design and level of style influence.
    
    strength is set to 0.45 to allow significant style changes while preserving room layout.
    guidance_scale of 7.5 increases adherence to the prompt.
    Negative prompt is fixed, as LLM's negative prompts are often low quality, describing 
    generic bad image features. Hence, using a higher quality fixed negative prompt 
    would yield higher image quality.

    
    """
    return pipe(
        prompt=prompt + ", real interior photograph, natural proportions",
        negative_prompt=(
            "cartoon, anime, illustration, painting, sketch, lowres, blurry, "
            "distorted geometry, warped walls, bent lines, duplicated furniture, "
            "bad perspective, fisheye distortion, oversharpened, noise, jpeg artifacts"
        ),
        image=init_image,
        strength=0.45,
        guidance_scale=7.5,
        num_inference_steps=25
    ).images[0]

router = APIRouter(prefix="/image", tags=["Image Generation"])

@router.post("/generate")
async def generate_image(
    file: UploadFile = File(...),
    styles: str = Form(...),
):
    try:
        # Save input image
        input_path = f"static/input_{uuid.uuid4()}.png"
        with open(input_path, "wb") as f:
            f.write(await file.read())

        init_image = Image.open(input_path).convert("RGB")

        # Generate prompt using LLM with retry logic
        max_retries = 3
        parsed = None

        for attempt in range(max_retries):
            try:
                llm_response = generate_sd_prompt(styles)
                parsed = llm_response
                if "prompt" not in parsed:
                    raise ValueError("Missing required fields in JSON")
                break
            except ValueError as e:
                print(f"Attempt {attempt + 1} failed: {str(e)}")
                if attempt == max_retries - 1:
                    # Use fallback prompts
                    print("All retries failed, using fallback prompts")
                    parsed = {
                        "prompt": f"{styles}, detailed, high quality, professional",
                        "negative": "cartoon, anime, illustration, painting, sketch, lowres, blurry, "
                                    "distorted geometry, warped walls, bent lines, duplicated furniture, "
                                    "bad perspective, fisheye distortion, oversharpened, noise, jpeg artifacts"
                    }
                    break

        prompt = parsed["prompt"]

        # Ensure prompts aren't empty
        if not prompt or prompt.strip() == "":
            prompt = "detailed, high quality, professional photograph"
        
        # Clear GPU cache before generation, helps with faster genration of images
        torch.cuda.empty_cache()

        base_size = choose_base_resolution(init_image)

        init_image = init_image.resize((base_size, base_size), Image.LANCZOS)

        # # Run Stable Diffusion img2img
        result = await run_in_threadpool( run_stable_diffusion, init_image, prompt)

        hires_image = result.resize(
            (result.width * 2, result.height * 2),
            Image.LANCZOS
        )

        hires_result = await run_in_threadpool(
            lambda: pipe(
                prompt=prompt + ", ultra detailed interior photograph, sharp focus, architectural photography",
                negative_prompt=(
                    "cartoon, anime, illustration, distorted geometry, warped walls, "
                    "duplicated furniture, bad perspective"
                ),
                image=hires_image,
                strength=0.20,         
                guidance_scale=7.0,
                num_inference_steps=30
            ).images[0]
        )
        output_path = f"static/output_{uuid.uuid4()}.png"
        hires_result.save(output_path, quality=95)  # Higher quality PNG

        return FileResponse(output_path)

    except Exception as e:
        print(f"Error in generate_image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))