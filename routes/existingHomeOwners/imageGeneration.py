"""
Image Generation API

This file handles image generation using img2img Stable Diffusion
It takes a input image and selected styles by the user, 
generates a text prompt using a LLM and then runs Stable Diffusion to generate
a new image

Workflow:
1. Receives input image and style preferences from frontend
2. Uses LLM to generate a prompt for Stable Diffusion based on the selected style(s)
3. Runs Stable Diffusion img2img to generate a new image
4. Returns the generated image file in the response

This API is used by the image generation feature in the frontend

Workflow Design Notes:
- Stable Diffusion parameters are fixed to balance image quality, realism and runtime
- img2img is used to preserve room layout and structure, instead of text-to-image
- Additional checks are added to ensure that LLM output is valid
"""

from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from fastapi.responses import FileResponse
from fastapi.concurrency import run_in_threadpool
from PIL import Image
import uuid
import json
import torch
from Services.StableDiffusion import pipe
from Services.LLMGemini import generate_sd_prompt

def run_stable_diffusion(init_image: Image.Image, prompt: str):
    """
    Run img2img Stable Diffusion with predefined generation parameters

    Args:
        init_image: room image uploaded by user
        prompt: text promp generated by LLM based on user selected style(s)

    Returns:
        Generated image based on the input image and prompt

    Generation Parameters (strength, guidance_scale) Explanation:
    These parameter values were chosen through experimentation to balance 
    generation time, image quality, realism for interior design and level of style influence.
    
    strength is set to 0.45 to allow significant style changes while preserving room layout.
    guidance_scale of 7.5 increases adherence to the prompt.
    Negative prompt is fixed, as LLM's negative prompts are often low quality, describing 
    generic bad image features. Hence, using a higher quality fixed negative prompt 
    would yield higher image quality.

    
    """
    return pipe(
        prompt=prompt + ", real interior photograph, natural proportions",
        negative_prompt=(
            "cartoon, anime, illustration, painting, sketch, lowres, blurry, "
            "distorted geometry, warped walls, bent lines, duplicated furniture, "
            "bad perspective, fisheye distortion, oversharpened, noise, jpeg artifacts"
        ),
        image=init_image,
        strength=0.45,
        guidance_scale=7.5,
        num_inference_steps=20
    ).images[0]

router = APIRouter(prefix="/image", tags=["Image Generation"])

@router.post("/generate")
async def generate_image(
    file: UploadFile = File(...),
    styles: str = Form(...),
):
    try:
        # Save input image
        input_path = f"static/input_{uuid.uuid4()}.png"
        with open(input_path, "wb") as f:
            f.write(await file.read())

        init_image = Image.open(input_path).convert("RGB")

        # Generate prompt using LLM with retry logic
        max_retries = 3
        parsed = None

        for attempt in range(max_retries):
            try:
                llm_response = generate_sd_prompt(styles)
                parsed = llm_response
                if "prompt" not in parsed or "negative" not in parsed:
                    raise ValueError("Missing required fields in JSON")
                break
            except ValueError as e:
                print(f"Attempt {attempt + 1} failed: {str(e)}")
                if attempt == max_retries - 1:
                    # Use fallback prompts
                    print("All retries failed, using fallback prompts")
                    parsed = {
                        "prompt": f"{styles}, detailed, high quality, professional",
                        "negative": "blurry, low quality, distorted, ugly, bad anatomy"
                    }
                    break

        prompt = parsed["prompt"]

        # Ensure prompts aren't empty
        if not prompt or prompt.strip() == "":
            prompt = "detailed, high quality, professional photograph"
        
        # Clear GPU cache before generation, helps with faster genration of images
        torch.cuda.empty_cache()

        # Resize input image to 512x512, increasing image size for better details 
        # or decreasing for faster generation. 
        init_image = init_image.resize((512, 512))

        # Run Stable Diffusion img2img
        result = await run_in_threadpool( run_stable_diffusion, init_image, prompt)
        output_path = f"static/output_{uuid.uuid4()}.png"
        result.save(output_path)

        return FileResponse(output_path)

    except Exception as e:
        print(f"Error in generate_image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))